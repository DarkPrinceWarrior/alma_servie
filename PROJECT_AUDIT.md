# Аудит проекта Alma Service

## 1. Общее впечатление
Проект представляет собой зрелый, хорошо структурированный пайплайн для обработки временных рядов и детекции аномалий. Код написан чисто, с использованием современных возможностей Python (type hinting, dataclasses). Видно, что архитектура продумывалась заранее: четкое разделение ответственности между модулями (`preprocessing`, `detection`, `simulation`) и конфигурацией.

Однако, проект находится в состоянии "написали свой велосипед для всего". Многие алгоритмы (Hampel filter, статистические тесты, валидация данных, CLI) реализованы вручную, хотя для этого существуют эффективные и проверенные open-source инструменты.

## 2. Что круто (Strong Points)
*   **Архитектура:** Четкое разделение на слои (загрузка -> предобработка -> фичи -> детекция -> отчеты).
*   **Конфигурация:** Использование YAML файла, который мапится в dataclass-ы — это отличная практика, позволяющая менять параметры без правки кода.
*   **Типизация:** Повсеместное использование аннотаций типов помогает в поддержке и понимании кода.
*   **Гибкость источников:** Абстракция `WorkbookSource` позволяет прозрачно переключаться между локальным Excel и HTTP API.
*   **Holdout-валидация:** Встроенная логика разделения на train/test и "честная" стриминговая симуляция (без подглядывания в будущее) — это редкость для DS-проектов "на коленке", здесь это сделано грамотно.

## 3. Критическая оценка пайплайна (Bottlenecks & Issues)

### 3.1. Производительность (Performance)
Самая большая проблема — **циклическая обработка данных в Python**.
*   **Стриминговая симуляция (`simulation.py`):** Метод `_simulate_interval` итерируется по точкам времени (`for evaluated_points, ts in enumerate...`). Это **катастрофически медленно** на больших объемах данных. В Pandas/Numpy нужно использовать векторизованные операции (expanding windows), а не циклы.
*   **Rolling Apply (`preprocessing.py`):** Использование `rolling(...).apply(lambda ...)` в Hampel-фильтре крайне неэффективно. Это не использует C-оптимизации Pandas.

### 3.2. "Самописные" алгоритмы
*   **Hampel Filter & Z-Score:** Реализованы вручную. Это увеличивает кодовую базу и риск багов.
*   **Hotelling T²:** Реализован на чистом NumPy.
*   **Валидация данных:** Проверки разбросаны по коду (`if df.empty`, `if col in df`). Нет единой схемы данных.

### 3.3. Инфраструктура
*   **Логирование:** Используются `print()` вместо нормального логгера. В продакшене это слепая зона — невозможно настроить уровни логирования, формат (JSON) или отправку в Sentry/ELK.
*   **CLI:** `argparse` — рабочий, но устаревший и многословный вариант.

## 4. Что устарело и чем заменить (Modernization Plan)

### 4.1. Библиотеки для Data Science и Аномалий
Вместо ручной реализации алгоритмов стоит использовать профильные библиотеки:

| Компонент | Текущая реализация | Рекомендованная замена | Почему? |
|-----------|--------------------|------------------------|---------|
| **Аномалии** | Custom Hampel, Z-score, T² | **PyOD** (Python Outlier Detection) или **ADTK** | Готовые, оптимизированные модели (включая PCA, IsolationForest, COPOD). ADTK отлично подходит для Time Series. |
| **Валидация** | `if/else` проверки | **Pandera** | Декларативное описание схем DataFrame. Валидация типов колонок, диапазонов значений и пропусков "из коробки". |
| **Временные ряды** | Pandas rolling/resample | **Darts** или **sktime** | (Опционально) Унифицированный интерфейс для работы с рядами, включая заполнение пропусков и прогнозирование. |

### 4.2. Инженерные библиотеки

| Компонент | Текущая реализация | Рекомендованная замена | Почему? |
|-----------|--------------------|------------------------|---------|
| **Конфиг** | PyYAML + Dataclasses | **Pydantic Settings** или **Hydra** | Строгая валидация типов конфига, поддержка переменных окружения (env vars), вложенность. |
| **CLI** | `argparse` | **Typer** (или Click) | Меньше бойлерплейта, автогенерация help, type conversion на основе аннотаций функций. |
| **Логирование** | `print()` | **Structlog** или **Loguru** | Структурированные JSON-логи, контекст (какая скважина сейчас обрабатывается), ротация файлов. |
| **Excel** | `openpyxl` | **Calamine** (python-calamine) | Если Excel файлы станут большими, Calamine (на Rust) читает их в разы быстрее. |

## 5. План рефакторинга (Roadmap)

1.  **Infrastructure First:**
    *   Заменить `print` на `loguru`.
    *   Внедрить `pandera` для валидации входных данных (особенно важно для API режима).

2.  **Performance Optimization:**
    *   Переписать `simulation.py` на векторизованный подход (используя `expanding().max()` и т.д. вместо цикла).
    *   Оптимизировать Hampel-фильтр (или заменить на библиотечный).

3.  **Core Logic Replacement:**
    *   Заменить ручной расчет T² и статистик на `PyOD` / `scipy.stats`.
    *   Это сократит объем кода в `detection.py` на 30-40%.

4.  **Parallelization:**
    *   Обработка скважин сейчас последовательная. Внедрить `joblib.Parallel` для независимой обработки скважин.

## 6. Вердикт
Проект хороший, но "академический". Для перевода в серьезный продакшен нужно избавиться от "детских болезней" (циклы в pandas, принты, ручные реализации мат.статистики) и переехать на промышленный стек библиотек.
