
Фаза 0: Предобработка (новое)
Цель: Очистить данные от шума и привести к сопоставимому виду
ДЛЯ каждой скважины:
  1. Ресэмплинг к единому шагу (если нужно)
  2. Hampel-фильтр на каждом канале → пометить/удалить точечные выбросы
  3. Forward-fill для небольших пропусков
```

**Интеграция с текущим кодом:**
- Добавить функцию `preprocess_well_data()` в `anomalies.py`
- Вызывать перед `compute_feature_frame()`
- Использовать библиотеку `scipy.stats` для Hampel или простую реализацию через rolling median

---

### Фаза 1: Частотная нормализация (обновление существующего)

**Цель:** Учесть, что нормальные значения Pressure/Current/Temperature зависят от Frequency
```
ДЛЯ каждого параметра (Pressure, Current, Temperature):
  
  НА референсной норме:
    1. Построить зависимость parameter = f(Frequency)
       Варианты:
       - Простой: биннинг по Frequency (шаг 2-5 Гц) + медиана/квантили
       - Продвинутый: локальная регрессия (LOWESS)
    
    2. Сохранить baseline_model[параметр] = функция или таблица
  
  НА всех данных:
    3. Вычислить expected = baseline_model(Frequency_текущая)
    4. residual = actual - expected
    5. MAD_scale = median(|residual - median(residual)|) * 1.4826
```

**Интеграция:**
- Расширить `compute_feature_frame()`: добавить параметр `frequency_normalized=True`
- Добавить функцию `build_frequency_baseline()` — строит модель по референсной норме
- Хранить baseline в конфиге или отдельном файле `baseline_models.pkl`

---

### Фаза 2: Улучшенная детекция (замена текущего алгоритма)

**Уровень 1: Одновариантные сигналы**
```
ДЛЯ каждого residual ряда:
  
  1. EWMA-чарт:
     z_t = λ × residual_t + (1-λ) × z_{t-1}
     
     Флаг если: z_t > L × MAD_scale
     
     Параметры: λ=0.2, L=3
  
  2. Точечные аномалии:
     Флаг если: |residual_t| > 4 × MAD_scale
     И это не одиночная точка (проверка соседей)
```

**Уровень 2: Мультивариантный мониторинг**
```
Вектор r_t = [residual_Pressure, residual_Current, residual_Temperature]

НА референсной норме:
  Вычислить ковариационную матрицу Σ

НА всех данных:
  T²_t = r_t^T × Σ^{-1} × r_t
  
  Флаг если: T²_t > порог (χ² распределение, 3 степени свободы, α=0.01)
```

**Уровень 3: Правила классификации**
```
ЕСЛИ обнаружен сигнал на любом уровне:
  
  1. Извлечь окно вокруг события (±30 минут)
  
  2. Вычислить паттерн:
     - Направления изменений (Pressure, Current, Temperature)
     - Последовательность (что изменилось раньше)
     - Соотношения (ΔPressure/ΔCurrent)
  
  3. Сопоставить с известными типами:
     
     Негерметичность НКТ:
       Pressure↑↑ И (Current↑ ИЛИ Current→) И Temperature↑_медленно
     
     Gas lock:
       Pressure↓↓ И Current↓ И волнообразность
     
     Перегрев:
       Temperature↑↑ И Current↑ устойчиво
  
  4. Если совпало → "известный тип"
     Если не совпало → "новая потенциальная аномалия"
```

**Интеграция:**
- Переписать `compute_rolling_detections()` с использованием EWMA вместо простого slope
- Добавить `compute_hotelling_t2()` для мультивариантного мониторинга
- Создать `classify_anomaly_type()` с правилами для известных типов

---

### Фаза 3: Постобработка (улучшение существующего)
```
1. Объединить близкие сегменты (уже есть merge_close_segments)
   
2. Фильтрация ложных срабатываний:
   - Минимальная длительность (уже есть min_duration_minutes)
   - Проверка валидности данных в окне
   - Удалить события в известных периодах простоя/техобслуживания

3. Вычислить severity для каждого события:
   S = (T² / порог_T²) × длительность_мин × max(|отклонения_параметров|)

4. Добавить confidence для классификации:
   - "высокая" — если паттерн четко совпал с известным типом
   - "средняя" — если T² высокий, но правила не сработали
   - "низкая" — если только EWMA на одном канале
```

**Интеграция:**
- Добавить поле `severity` и `confidence` в выходной DataFrame
- Обновить `merge_close_segments()` с учетом severity

---

### Фаза 4: Референсное сопоставление (опциональное улучшение)

**Только для сложных случаев, где правила не дали результат**
```
ЕСЛИ confidence == "средняя" ИЛИ "низкая":
  
  1. Извлечь эталонные формы из референсных аномалий
     (z-нормированные окна)
  
  2. Вычислить feature-based similarity:
     - Корреляция форм
     - Cosine similarity векторов [ΔP, ΔI, ΔT]
     - Euclidean distance в feature space
  
  3. ЕСЛИ similarity > порог:
     Обновить тип и confidence = "высокая"
```

**Интеграция:**
- Добавить функцию `compare_with_reference_patterns()`
- Вызывать только для неопределенных случаев
- НЕ использовать DTW (слишком медленно), использовать простые метрики

---

## План внедрения

### Этап 1: Базовые улучшения
**Задачи:**
1. Добавить Hampel-фильтр в предобработку
2. Заменить `std()` на `MAD` во всех вычислениях порогов
3. Обновить `compute_feature_frame()` для работы с residuals

**Ожидаемый результат:**
- Меньше ложных срабатываний от шума
- Более устойчивые пороги

**Изменения в коде:**
- `anomalies.py`: новая функция `robust_scale()` и `hampel_filter()`
- Обновить `compute_thresholds()` для использования MAD

---

### Этап 2: Частотная нормализация
**Задачи:**
1. Реализовать `build_frequency_baseline()` — обучение на референсной норме
2. Добавить вычисление residuals в `compute_feature_frame()`
3. Сохранять baseline models в `reports/anomalies/baseline_models.pkl`

**Ожидаемый результат:**
- Детектор учитывает режим работы (частоту)
- Меньше ложных срабатываний при изменении частоты

**Изменения в коде:**
- `anomalies.py`: новые функции для baseline
- `config.py`: добавить параметры `frequency_bins` или `lowess_frac`
- Обновить `run_anomaly_analysis()` для предварительного обучения baseline

---

### Этап 3: EWMA и мультивариантный мониторинг
**Задачи:**
1. Реализовать `compute_ewma_chart()` для каждого residual ряда
2. Реализовать `compute_hotelling_t2()` для векторов residuals
3. Обновить логику детекции: комбинировать EWMA + T²

**Ожидаемый результат:**
- Раннее обнаружение медленных трендов
- Лучшее обнаружение комплексных аномалий

**Изменения в коде:**
- `anomalies.py`: новые функции для EWMA и T²
- Обновить `compute_rolling_detections()` для интеграции новых методов
- Добавить параметры `ewma_lambda`, `t2_alpha` в `config/pipeline.yaml`

---

### Этап 4: Классификация типов аномалий
**Задачи:**
1. Создать `classify_anomaly_type()` с правилами для известных типов
2. Добавить вычисление `severity` и `confidence`
3. Обновить выходные файлы (добавить колонки `anomaly_type`, `severity`, `confidence`)

**Ожидаемый результат:**
- Автоматическая идентификация типа аномалии
- Приоритизация событий по severity

**Изменения в коде:**
- `anomalies.py`: новая функция с доменными правилами
- Обновить схему выходного DataFrame
- Обновить `generate_reference_report.py` для отображения типов

---

### Этап 5: Референсное сопоставление (опционально)
**Задачи:**
1. Реализовать `extract_reference_patterns()` — извлечение эталонов из svod
2. Реализовать `compare_with_reference_patterns()` — feature-based similarity
3. Интегрировать в пайплайн только для неопределенных случаев

**Ожидаемый результат:**
- Улучшенная классификация сложных случаев
- Обнаружение редких типов аномалий

**Изменения в коде:**
- Новый модуль `pattern_matching.py`
- Опциональный параметр `use_pattern_matching` в конфиге

---

## Рекомендуемые библиотеки

### Tier 1: Обязательно добавить

**1. scipy** (для робастной статистики и фильтров)
```
pip install scipy
```
- `scipy.stats.median_abs_deviation` — MAD
- `scipy.signal.medfilt` — медианная фильтрация
- Hampel можно реализовать самостоятельно через rolling

**2. scikit-learn** (для мультивариантного мониторинга)
```
pip install scikit-learn
```
- `sklearn.covariance.EmpiricalCovariance` — для Hotelling T²
- `sklearn.preprocessing.StandardScaler` — для z-нормализации
- Уже популярная, хорошо документированная

**3. statsmodels** (для EWMA и временных рядов)
```
pip install statsmodels
```
- `statsmodels.tsa.stattools.acf` — автокорреляция
- `statsmodels.nonparametric.smoothers_lowess.lowess` — для baseline по частоте
- Канонический инструмент для статистического анализа

---

### Tier 2: Желательно добавить

**4. adtk** (Anomaly Detection Toolkit)
```
pip install adtk
```
- `adtk.detector.VolatilityShiftAD` — детектор изменений volatility
- `adtk.detector.PersistAD` — детектор устойчивых сдвигов
- `adtk.transformer.RollingAggregate` — готовые агрегации
- **Преимущество:** специально для временных рядов, простой API, пайплайны

**5. stumpy** (Matrix Profile)
```
pip install stumpy
```
- `stumpy.stump()` — Matrix Profile для поиска паттернов
- `stumpy.fluss()` — поиск изменений режима
- **Преимущество:** очень быстрый, хорош для поиска повторяющихся/аномальных паттернов

---

### Tier 3: Для продвинутых возможностей (опционально)

**6. ruptures** (Change Point Detection)
```
pip install ruptures
```
- `ruptures.Pelt()` — PELT алгоритм
- `ruptures.BottomUp()` — Bottom-Up сегментация
- **Использовать если:** нужно находить резкие смены режимов

**7. tslearn** (Time Series Machine Learning)
```
pip install tslearn
```
- `tslearn.metrics.dtw` — Dynamic Time Warping
- `tslearn.clustering.TimeSeriesKMeans` — кластеризация временных рядов
- **Использовать если:** решите делать DTW-сопоставление

**8. PyOD** (Python Outlier Detection)
```
pip install pyod
```
- Много методов из коробки (LOF, COPOD, etc.)
- Хорош для одновариантных/мультивариантных выбросов
- **Использовать если:** хотите экспериментировать с разными детекторами

---

### Tier 4: Специализированные (если понадобятся)

**9. control-charts** или **spc**
```
pip install spc
```
- Готовые CUSUM, EWMA, Shewhart charts
- Western Electric rules
- **Использовать если:** хотите канонические SPC методы без реализации

**10. luminol** (от LinkedIn)
```
pip install luminol
```
- Correlation-based аномалии
- Root cause analysis
- **Использовать если:** нужен анализ взаимосвязей между скважинами

---

## Минимальный набор для старта

Для первых 3 этапов достаточно:
```
scipy>=1.11.0
scikit-learn>=1.3.0
statsmodels>=0.14.0
```

Если захотите использовать готовые time series детекторы:
```
+ adtk>=0.6.2
+ stumpy>=1.12.0