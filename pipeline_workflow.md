# Пайплайн определения аномалий по временным рядам скважин

Этот документ описывает полный процесс обработки сырых замеров АГЗУ и СУ до получения решения, является ли событие аномалией по заданным условиям.

## 1. Роль конфигурации

Файл `config/pipeline.yaml` содержит ключевые настройки:
- пути для хранения данных (`data/raw`, `data/interim`, `data/processed`, `reports`);
- колонки, ожидаемые в листах АГЗУ и СУ, названия столбцов с датой и идентификатором скважины;
- правила заполнения пропусков и фильтров выбросов;
- параметры ресэмплинга (часовой шаг, максимальная «протяжка» значений);
- список показателей для расчёта отклонений и префиксы `АГЗУ_*`, `СУ_*` в итоговом отчёте;
- блок `anomalies.conditions` — набор правил (оператор, порог, список метрик), определяющих, когда фиксируется аномалия.

Все дальнейшие шаги ссылаются на эти параметры.

## 2. Подготовка исходных данных

1. **Сбор файлов**: положите все Excel-файлы со скважинами в `data/raw`. Желательны имена вида `<well_number>.xlsx`.
2. **Ингеста (опционально)**: если файлы лежат в другой директории, используйте
   ```bash
   source venv/bin/activate
   PYTHONPATH=src python -m pipeline ingest --source /путь/к/xlsx
   ```
   Команда скопирует файлы в `data/raw` и обновит `data/raw/manifest.csv` с метаданными (хэши, время выгрузки).

## 3. Построение промежуточных таблиц

1. **Разбор Excel по скважинам**:
   ```bash
   source venv/bin/activate
   PYTHONPATH=src python scripts/run_extract.py --wells 1018,339
   ```
   Скрипт читает листы `Замеры АГЗУ из ТМ` и `Замеры СУ`, сохраняет пофайловые parquet в `data/interim/agzu_wells/` и `data/interim/su_wells/`, логирует предупреждения о пропусках листов/колонок в `data/interim/raw_validation.json`. При большом объёме данных запускайте несколькими батчами, чтобы избежать таймаута.

2. **Объединение по всем скважинам**:
   ```bash
   PYTHONPATH=src python -m pipeline extract --combine-only
   ```
   Формируются `data/interim/agzu_raw.parquet` и `data/interim/su_raw.parquet` – сводные таблицы без очистки.

## 4. Контроль качества и очистка

1. **QC-отчёты**:
   ```bash
   PYTHONPATH=src python -m pipeline qc
   ```
   В `reports/qc/` появятся HTML-страницы с разрывами по времени, дубликатами, отрицательными значениями, долями нулей и NaN – используйте для согласования правил.

2. **Очистка**:
   ```bash
   PYTHONPATH=src python -m pipeline clean
   ```
   Выполняется:
   - сортировка по времени для каждой скважины;
   - замена отрицательных дебитов и давлений АГЗУ на `NaN` + обнуление ненадёжных длительностей > порога;
   - преобразование числовых колонок СУ в float, заполнение `NaN` методом ffill→bfill внутри скважины;
   - запись результатов в `data/processed/agzu_clean.parquet` и `data/processed/su_clean.parquet`;
   - сводка по очистке в `reports/cleaning_summary.json` (сколько значений заменено, доля нулей).

## 5. Приведение к общей временной сетке

```bash
PYTHONPATH=src python -m pipeline align
```
- АГЗУ и СУ ресэмплируются на часовой шаг (`frequency: "1H"`), допускается протягивание значения не более `agzu_max_ffill_hours` и `su_max_ffill_hours` часов соответственно (превышение → `NaN`).
- Для СУ перед протяжкой вычисляется среднее внутри каждого часа.
- Итог: `data/processed/agzu_resampled.parquet`, `data/processed/su_resampled.parquet` и общий `data/processed/merged_hourly.parquet` с колонкой `timestamp` и показателями обеих систем.

## 6. Подготовка списка аномалий

- Файл `alma/Общая таблица.xlsx` содержит таблицу «Ненормальная работа» с указанием скважины, названия события и диапазона дат в формате `ДД.ММ.ГГ ЧЧ:ММ - ДД.ММ.ГГ ЧЧ:ММ`.
- При необходимости можно указать иной путь флагом `--source` у команды аномалий.

### Анализ референсных эпизодов

Дополнительно в этой же книге есть лист «Нормальная работа». Команда
```bash
PYTHONPATH=src python -m pipeline events
```
парсит оба листа, рассчитывает такие же признаки (проценты изменений) и
выгружает:
- объединённую выборку с метками `label ∈ {normal, abnormal}`:
  `reports/anomalies/events_features.parquet` (и .csv);
- статистику по метрикам и событиям с перцентилями (`events_summary.json`).

Эти отчёты служат опорой для калибровки порогов: можно подобрать такие условия,
чтобы «normal» периоды оставались внутри допустимых интервалов, а «abnormal» их превышали.

## 7. Расчёт отклонений и оценка условий

Запустите:
```bash
PYTHONPATH=src python -m pipeline anomalies
```
Команда выполняет:
1. Парсинг событий и приведение номеров скважин к строковому типу.
2. Для каждого события выбираются данные merged-таблицы за `pre_window_days` (по умолчанию три дня) до начала и за период аномалии.
3. Для метрик из списков `anomalies.agzu_metrics` и `anomalies.su_metrics` рассчитываются средние значения в двух окнах и процентное изменение (если базовое среднее равно нулю, используются оговорённые исключения: 0→0 = 0%, 0→≠0 = 100%).
4. Формируются индикаторы `АГЗУ_точек_до` и `АГЗУ_точек_во_время` — число точек в каждом окне.
5. Применяются правила из `anomalies.conditions`. Пример одного условия:
   ```yaml
   anomalies:
     conditions:
       - name: "low_pressure"
         rules:
           - metric: "СУ_Давление на приеме насоса_изм%"
             operator: "<="
             threshold: -15
           - metric: "СУ_Коэффициент загрузки_изм%"
             operator: "<="
             threshold: -10
   ```
   Условие считается выполненным, если все правила внутри него истинны (значение существует и удовлетворяет оператору). В итоговой таблице появится колонка `cond_low_pressure` со значением `True/False`.

6. Результаты сохраняются:
   - `reports/anomalies/anomaly_analysis.parquet`
   - `reports/anomalies/anomaly_analysis.xlsx`
   - `reports/anomalies/anomaly_analysis.json`

**Интерпретация**: если для события хотя бы одна колонка `cond_*` равна `True`, условия аномалии выполнены. При отсутствии условий (пустой список) таблица содержит только проценты отклонений, дальнейшее решение принимается вручную или по внешним правилам.

## 8. Полный прогон «одной кнопкой»

При необходимости можно выполнить все шаги подряд (кроме отдельной батчевой разбивки):
```bash
PYTHONPATH=src python -m pipeline full \
  --source /путь/к/raw_excel   # опционально
  --anomaly-source /путь/к/Общая_таблица.xlsx  # опционально
```
Команда последовательно выполнит `ingest` (если задан source), `extract`, `clean`, `align`, `anomalies`.

## 9. Проверка и доработка условий

- Всегда просматривайте QC-отчёты и `reports/cleaning_summary.json`, чтобы убедиться в корректности очистки.
- Перед добавлением новых правил оцените распределение показателей в «до» и «во время»; при необходимости допишите проверки в конфиг.
- После изменения `config/pipeline.yaml` перепустите соответствующие шаги (например, при новых порогах аномалий достаточно перезапустить `python -m pipeline anomalies`).

## 10. Минимальный чек лист перед расчётом аномалий

1. В `data/raw` лежат актуальные Excel по всем скважинам.
2. В `alma/Общая таблица.xlsx` – актуальный список событий.
3. Команды `extract --combine-only`, `clean`, `align` отрабатывают без ошибок, а `merged_hourly.parquet` содержит данные по интересующим датам.
4. В `config/pipeline.yaml` заданы актуальные условия аномалий.
5. После запуска `python -m pipeline anomalies` проверены итоговые файлы в `reports/anomalies/`.
6. (По необходимости) Выполнен `python -m pipeline events`, обновлены отчёты `events_features.*` и `events_summary.json`, а пороги в `config/pipeline.yaml` согласованы по новой статистике.

Следуя этому чек-листу, вы получаете воспроизводимый ответ: выполнены ли условия аномалии для каждой скважины в заданные периоды.
