# Пайплайн определения аномалий по временным рядам скважин

Этот документ описывает полный процесс обработки сырых замеров АГЗУ и СУ до получения решения, является ли событие аномалией по заданным условиям.

## 1. Роль конфигурации

Файл `config/pipeline.yaml` содержит ключевые настройки:
- пути для хранения данных (`data/raw`, `data/interim`, `data/processed`, `reports`);
- колонки, ожидаемые в листах АГЗУ и СУ, названия столбцов с датой и идентификатором скважины;
- правила заполнения пропусков и фильтров выбросов;
- параметры временной сетки (например, часовой шаг для выравнивания);
- список показателей для расчёта отклонений и префиксы `АГЗУ_*`, `СУ_*` в итоговом отчёте;
- блок `anomalies.detection` — окно расчёта скользящих метрик, смещение базового окна и т.д.;
- блок `anomalies.rules` — перечень именованных правил с порогами, длительностью и приоритетными показателями.

Все дальнейшие шаги ссылаются на эти параметры.

## 2. Подготовка исходных данных

1. **Сбор файлов**: положите все Excel-файлы со скважинами в `data/raw`. Желательны имена вида `<well_number>.xlsx`.
2. **Ингеста (опционально)**: если файлы лежат в другой директории, используйте
   ```bash
   source venv/bin/activate
   PYTHONPATH=src python -m pipeline ingest --source /путь/к/xlsx
   ```
   Команда скопирует файлы в `data/raw` и обновит `data/raw/manifest.csv` с метаданными (хэши, время выгрузки).

## 3. Построение промежуточных таблиц

1. **Разбор Excel по скважинам**:
   ```bash
   source venv/bin/activate
   PYTHONPATH=src python scripts/run_extract.py --wells 1018,339
   ```
   Скрипт читает листы `Замеры АГЗУ из ТМ` и `Замеры СУ`, сохраняет пофайловые parquet в `data/interim/agzu_wells/` и `data/interim/su_wells/`, логирует предупреждения о пропусках листов/колонок в `data/interim/raw_validation.json`. При большом объёме данных запускайте несколькими батчами, чтобы избежать таймаута.

2. **Объединение по всем скважинам**:
   ```bash
   PYTHONPATH=src python -m pipeline extract --combine-only
   ```
   Формируются `data/interim/agzu_raw.parquet` и `data/interim/su_raw.parquet` – сводные таблицы без очистки.

## 4. Контроль качества и очистка

1. **QC-отчёты**:
   ```bash
   PYTHONPATH=src python -m pipeline qc
   ```
   В `reports/qc/` появятся HTML-страницы с разрывами по времени, дубликатами, отрицательными значениями, долями нулей и NaN – используйте для согласования правил.

2. **Очистка**:
   ```bash
   PYTHONPATH=src python -m pipeline clean
   ```
   Выполняется:
   - сортировка по времени для каждой скважины;
   - замена отрицательных дебитов и давлений АГЗУ на `NaN` + обнуление ненадёжных длительностей > порога;
   - преобразование числовых колонок СУ в float, заполнение `NaN` методом ffill→bfill внутри скважины;
   - запись результатов в `data/processed/agzu_clean.parquet` и `data/processed/su_clean.parquet`;
   - сводка по очистке в `reports/cleaning_summary.json` (сколько значений заменено, доля нулей).

## 5. Приведение к общей временной сетке

```bash
PYTHONPATH=src python -m pipeline align
```
- Для каждой скважины формируется часовой ряд от первого до последнего замера. Значения внутри часа усредняются (у СУ это сглаживает десятки точек), затем по часовому индексу выполняется `interpolate(method='time')`, чтобы заполнить разрывы. Там, где в `_clean`‑таблицах целые часы были пустыми, `NaN` сохраняются.
- Полученные часовые ряды АГЗУ и СУ выравниваются по общему диапазону: если одна система начиналась раньше или заканчивалась позже, у второй достраиваются недостающие часы строками с `NaN`.
- Итог: `data/processed/agzu_resampled.parquet`, `data/processed/su_resampled.parquet` и общий `data/processed/merged_hourly.parquet` с колонкой `timestamp` и признаками обеих систем, синхронизированными по часам.

## 6. Подготовка списка аномалий

- Файл `alma/Общая таблица.xlsx` содержит таблицу «Ненормальная работа» с указанием скважины, названия события и диапазона дат в формате `ДД.ММ.ГГ ЧЧ:ММ - ДД.ММ.ГГ ЧЧ:ММ`.
- При необходимости можно указать иной путь флагом `--source` у команды аномалий.

### Анализ референсных эпизодов

Дополнительно в этой же книге есть лист «Нормальная работа». Команда
```bash
PYTHONPATH=src python -m pipeline events
```
парсит оба листа, рассчитывает такие же признаки (проценты изменений) и
выгружает:
- объединённую выборку с метками `label ∈ {normal, abnormal}`:
  `reports/anomalies/events_features.parquet` (и .csv);
- статистику по метрикам и событиям с перцентилями (`events_summary.json`).

Эти отчёты служат опорой для калибровки порогов: можно подобрать такие правила,
чтобы «normal» периоды оставались внутри допустимых интервалов, а «abnormal» их превышали.

## 7. Правила детекции аномалий

Запустите:
```bash
PYTHONPATH=src python -m pipeline anomalies
```
Команда выполняет последовательность действий:

1. Для каждой скважины вычисляются скользящие признаки по списку `anomalies.detection.metrics`: текущие и базовые (со смещением) средние, стандартные отклонения, минимумы/максимумы, доля валидных точек, относительное изменение `__pct_change` и абсолютное отличие `__delta`. Ширина окна и смещение задаются параметрами `window_hours` и `baseline_shift_hours`.
2. Для каждого правила из `anomalies.rules` строится булева маска по условиям `all`, `any`, `none`. Поддерживаются операторы `>=`, `>`, `<=`, `<`, `between`, `outside`, `isna`, `notna`, а также флаг `abs: true` для сравнения по модулю.
3. Маски преобразуются в интервалы: учитываются только подряд идущие `True` длиной не меньше `min_duration_hours`; после фиксации эпизода применяется `cooldown_hours`, чтобы соседние срабатывания не дублировались.
4. Для каждого эпизода собирается агрегированная статистика по `focus_metrics`: средние и экстремальные значения `__pct_change`, `__delta`, доля валидных точек, число сэмплов, оценка пика (`peak_time`) и текстовое резюме `notes`.

Пример фрагмента конфигурации:

```yaml
anomalies:
  detection:
    window_hours: 6
    baseline_shift_hours: 6
    min_valid_fraction: 0.6
  rules:
    - name: pressure_drop_constant_freq
      label: "Падение давления на приеме при постоянной частоте"
      min_duration_hours: 3
      cooldown_hours: 2
      focus_metrics:
        - "Давление на приеме насоса"
        - "Выходная частота"
      all:
        - feature: "Давление на приеме насоса__pct_change"
          operator: "<="
          value: -12
        - feature: "Давление на приеме насоса__delta"
          operator: "<="
          value: -2
        - feature: "Выходная частота__pct_change"
          operator: "between"
          min: -3
          max: 3
```

Результаты сохраняются в:
- `reports/anomalies/anomaly_analysis.parquet`
- `reports/anomalies/anomaly_analysis.xlsx`
- `reports/anomalies/anomaly_analysis.json`

Ключевые поля отчёта: `rule_name`, `rule_label`, `start`, `end`, `duration_hours`, `score` (среднее абсолютное отклонение по фокусным метрикам), `notes`. Остальные колонки — агрегаты по конкретным метрикам (`<метрика>__pct_mean`, `<метрика>__delta_mean`, `<метрика>__valid_fraction_mean` и т.д.). Если список правил пуст, детектор вернёт пустую таблицу.

## 8. Полный прогон «одной кнопкой»

При необходимости можно выполнить все шаги подряд (кроме отдельной батчевой разбивки):
```bash
PYTHONPATH=src python -m pipeline full \
  --source /путь/к/raw_excel   # опционально
  --anomaly-source /путь/к/Общая_таблица.xlsx  # опционально; используется только для шага events
```
Команда последовательно выполнит `ingest` (если задан source), `extract`, `clean`, `align`, `anomalies`.

## 9. Проверка и доработка правил

- Всегда просматривайте QC-отчёты и `reports/cleaning_summary.json`, чтобы убедиться в корректности очистки.
- Перед настройкой новых правил оцените распределение показателей в «до» и «во время»; при необходимости скорректируйте пороги в конфигурации и пересчитайте `events_features`.
- После изменения `config/pipeline.yaml` перепустите соответствующие шаги (например, при новых порогах аномалий достаточно перезапустить `python -m pipeline anomalies`).

## 10. Минимальный чек лист перед расчётом аномалий

1. В `data/raw` лежат актуальные Excel по всем скважинам.
2. (Опционально) В `alma/Общая таблица.xlsx` — актуальные примеры для калибровки правил через `python -m pipeline events`.
3. Команды `extract --combine-only`, `clean`, `align` отрабатывают без ошибок, а `merged_hourly.parquet` содержит данные по интересующим датам.
4. В `config/pipeline.yaml` заданы актуальные правила `anomalies.rules` и параметры `anomalies.detection`.
5. После запуска `python -m pipeline anomalies` проверены итоговые файлы в `reports/anomalies/`.
6. (По необходимости) Выполнен `python -m pipeline events`, обновлены отчёты `events_features.*` и `events_summary.json`, а пороги в `config/pipeline.yaml` согласованы по новой статистике.

Следуя этому чек-листу, вы получаете воспроизводимый ответ: какие правила аномалий сработали для каждой скважины в заданные периоды.
